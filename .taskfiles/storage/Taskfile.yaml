---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

# Taskfile for managing storage class migrations
# Primary use case: Migrating PVCs from mayastor-single-replica to cephfs-shared (and later to ceph-rbd)

vars:
  KUBECTL: kubectl

tasks:

  list-mayastor:
    desc: List all PVCs still using mayastor-single-replica storage class
    summary: |
      Shows which PVCs are still on mayastor storage to help track migration progress.
      Groups by namespace for easier planning.
    cmds:
      - |
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "PVCs using mayastor-single-replica storage class:"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        PVCS=$({{.KUBECTL}} get pvc -A -o json | jq -r '.items[] | select(.spec.storageClassName=="mayastor-single-replica") | "\(.metadata.namespace)/\(.metadata.name)"' | sort)

        if [ -z "$PVCS" ]; then
          echo "âœ… No PVCs found using mayastor-single-replica!"
          echo "   All PVCs have been migrated off Mayastor."
        else
          COUNT=$(echo "$PVCS" | wc -l)
          echo "$PVCS" | awk -F/ '{print "  " $1 "/" $2}'
          echo ""
          echo "Total: $COUNT PVCs still on mayastor-single-replica"
          echo ""
          echo "Run: task storage:migrate-pvc APP=<app> NS=<namespace> STORAGE_CLASS=cephfs-shared"
        fi
        echo ""

  verify-volsync:
    desc: Check volsync backup status for all apps
    summary: |
      Lists all ReplicationSources and their last sync time.
      Use this to verify apps have recent backups before migrating.
    cmds:
      - |
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "Volsync Backup Status (ReplicationSources):"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
        printf "%-20s %-25s %-25s %s\n" "NAMESPACE" "APP" "LAST SYNC" "STATUS"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

        {{.KUBECTL}} get replicationsources -A -o json | jq -r '.items[] |
          [.metadata.namespace, .metadata.name, (.status.lastSyncTime // "Never"), (.status.lastSyncDuration // "N/A")] |
          @tsv' | while IFS=$'\t' read -r ns name sync duration; do
          printf "%-20s %-25s %-25s %s\n" "$ns" "$name" "$sync" "$duration"
        done
        echo ""

  show-progress:
    desc: Show migration progress - apps migrated vs remaining
    summary: |
      Displays which apps have been migrated to cephfs-shared (have overrides in ks.yaml)
      and which are still using default (mayastor).
    cmds:
      - |
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "Storage Migration Progress:"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        MIGRATED=0
        PENDING=0

        echo "Apps with storage class override (migrated to cephfs-shared):"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

        # Find all ks.yaml files with VOLSYNC_STORAGECLASS override
        for ks_file in kubernetes/apps/*/*/ks.yaml; do
          if [ -f "$ks_file" ]; then
            if grep -q "VOLSYNC_STORAGECLASS.*cephfs-shared" "$ks_file" 2>/dev/null; then
              app=$(basename $(dirname "$ks_file"))
              ns=$(basename $(dirname $(dirname "$ks_file")))
              echo "  âœ… $ns/$app"
              MIGRATED=$((MIGRATED + 1))
            fi
          fi
        done

        echo ""
        echo "Apps still using default storage class (pending migration):"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

        # Find apps with volsync component but no override
        for ks_file in kubernetes/apps/*/*/ks.yaml; do
          if [ -f "$ks_file" ]; then
            # Check if uses volsync component and has VOLSYNC_CAPACITY
            if grep -q "VOLSYNC_CAPACITY" "$ks_file" 2>/dev/null; then
              # Check if doesn't have cephfs override
              if ! grep -q "VOLSYNC_STORAGECLASS.*cephfs-shared" "$ks_file" 2>/dev/null; then
                app=$(basename $(dirname "$ks_file"))
                ns=$(basename $(dirname $(dirname "$ks_file")))
                echo "  â³ $ns/$app"
                PENDING=$((PENDING + 1))
              fi
            fi
          fi
        done

        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        TOTAL=$((MIGRATED + PENDING))
        PERCENT=0
        if [ $TOTAL -gt 0 ]; then
          PERCENT=$((MIGRATED * 100 / TOTAL))
        fi
        echo "Progress: $MIGRATED/$TOTAL apps migrated ($PERCENT%)"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

  migrate-pvc:
    desc: Migrate a single app's PVC to a new storage class [APP=required] [NS=required] [STORAGE_CLASS=cephfs-shared] [SNAPSHOT_CLASS=cephfs-snapshot] [COMMIT=false]
    summary: |
      Migrates an app's PVC to a new storage class by:
      1. Validating prerequisites (app exists, has volsync backup, etc.)
      2. Adding storage class override to app's ks.yaml
      3. Optionally committing and pushing changes (set COMMIT=true)
      4. Scaling down the app
      5. Waiting for PVC deletion and recreation with volsync restore
      6. Scaling app back up
      7. Verifying migration succeeded

      Examples:
        Manual commit: task storage:migrate-pvc APP=bazarr NS=media STORAGE_CLASS=cephfs-shared
        Auto commit:   task storage:migrate-pvc APP=bazarr NS=media STORAGE_CLASS=cephfs-shared COMMIT=true
    vars:
      NS: '{{.NS | default "default"}}'
      STORAGE_CLASS: '{{.STORAGE_CLASS | default "cephfs-shared"}}'
      SNAPSHOT_CLASS: '{{.SNAPSHOT_CLASS | default "cephfs-snapshot"}}'
      COMMIT: '{{.COMMIT | default "false"}}'
      KS_FILE: '{{.ROOT_DIR}}/kubernetes/apps/{{.NS}}/{{.APP}}/ks.yaml'
      CONTROLLER:
        sh: '{{.KUBECTL}} --namespace {{.NS}} get deployment {{.APP}} &>/dev/null && echo deployment || echo statefulset'
      ORIGINAL_REPLICAS:
        sh: '{{.KUBECTL}} --namespace {{.NS}} get {{.CONTROLLER}}/{{.APP}} -o jsonpath="{.spec.replicas}" 2>/dev/null || echo "1"'
    requires:
      vars: [APP, NS]
    preconditions:
      - sh: '[ -f "{{.KS_FILE}}" ]'
        msg: 'Kustomization file not found at {{.KS_FILE}}'
      - sh: '{{.KUBECTL}} --namespace {{.NS}} get {{.CONTROLLER}}/{{.APP}} &>/dev/null'
        msg: 'App {{.APP}} not found in namespace {{.NS}}'
      - sh: '{{.KUBECTL}} --namespace {{.NS}} get replicationsource {{.APP}} &>/dev/null'
        msg: 'No volsync ReplicationSource found for {{.APP}} - cannot safely migrate'
      - sh: '{{.KUBECTL}} get storageclass {{.STORAGE_CLASS}} &>/dev/null'
        msg: 'Storage class {{.STORAGE_CLASS}} does not exist'
    cmds:
      - task: _validate-backup
        vars: {APP: '{{.APP}}', NS: '{{.NS}}'}
      - task: _commit-or-wait
        vars: {APP: '{{.APP}}', NS: '{{.NS}}', STORAGE_CLASS: '{{.STORAGE_CLASS}}', SNAPSHOT_CLASS: '{{.SNAPSHOT_CLASS}}', KS_FILE: '{{.KS_FILE}}', COMMIT: '{{.COMMIT}}'}
      - task: _scale-down
        vars: {APP: '{{.APP}}', NS: '{{.NS}}', CONTROLLER: '{{.CONTROLLER}}'}
      - task: _wait-for-pvc-recreation
        vars: {APP: '{{.APP}}', NS: '{{.NS}}', STORAGE_CLASS: '{{.STORAGE_CLASS}}'}
      - task: _scale-up
        vars: {APP: '{{.APP}}', NS: '{{.NS}}', CONTROLLER: '{{.CONTROLLER}}', REPLICAS: '{{.ORIGINAL_REPLICAS}}'}
      - task: _verify-migration
        vars: {APP: '{{.APP}}', NS: '{{.NS}}', STORAGE_CLASS: '{{.STORAGE_CLASS}}'}
      - echo "âœ… Migration complete for {{.APP}}"

  # Internal task: Validate backup exists and is recent
  _validate-backup:
    internal: true
    vars:
      LAST_SYNC:
        sh: '{{.KUBECTL}} --namespace {{.NS}} get replicationsource {{.APP}} -o jsonpath="{.status.lastSyncTime}" 2>/dev/null || echo ""'
    cmds:
      - |
        if [ -z "{{.LAST_SYNC}}" ]; then
          echo "âŒ No recent backup found for {{.APP}}"
          echo "   Run: task volsync:snapshot APP={{.APP}} NS={{.NS}}"
          exit 1
        fi
        echo "âœ… Recent backup found: {{.LAST_SYNC}}"

  # Internal task: Add storage overrides and optionally commit
  _commit-or-wait:
    internal: true
    cmds:
      - |
        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "ğŸ“ Updating Kustomization with storage class overrides"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        # Check if yq is available
        if ! command -v yq &>/dev/null; then
          echo "âŒ yq not found. Please install yq to use this task."
          echo "   Or manually edit {{.KS_FILE}} and add:"
          echo ""
          echo "  spec:"
          echo "    postBuild:"
          echo "      substitute:"
          echo "        VOLSYNC_STORAGECLASS: {{.STORAGE_CLASS}}"
          echo "        VOLSYNC_SNAPSHOTCLASS: {{.SNAPSHOT_CLASS}}"
          echo "        VOLSYNC_CACHE_STORAGECLASS: {{.STORAGE_CLASS}}"
          exit 1
        fi

        # Add storage class overrides using yq
        echo "   Adding VOLSYNC_STORAGECLASS: {{.STORAGE_CLASS}}"
        yq eval '.spec.postBuild.substitute.VOLSYNC_STORAGECLASS = "{{.STORAGE_CLASS}}"' -i {{.KS_FILE}}

        echo "   Adding VOLSYNC_SNAPSHOTCLASS: {{.SNAPSHOT_CLASS}}"
        yq eval '.spec.postBuild.substitute.VOLSYNC_SNAPSHOTCLASS = "{{.SNAPSHOT_CLASS}}"' -i {{.KS_FILE}}

        echo "   Adding VOLSYNC_CACHE_STORAGECLASS: {{.STORAGE_CLASS}}"
        yq eval '.spec.postBuild.substitute.VOLSYNC_CACHE_STORAGECLASS = "{{.STORAGE_CLASS}}"' -i {{.KS_FILE}}

        echo "âœ… Updated {{.KS_FILE}}"
        echo ""

        if [ "{{.COMMIT}}" = "true" ]; then
          echo "ğŸš€ Auto-commit enabled - committing and pushing changes..."
          echo ""

          # Stage the file
          git add {{.KS_FILE}}

          # Commit
          git commit -m "chore({{.NS}}): migrate {{.APP}} PVC to {{.STORAGE_CLASS}}" \
            -m "Migrating {{.APP}} storage from mayastor to {{.STORAGE_CLASS}}." \
            -m "This updates volsync configuration to use the new storage class and snapshot class for backups and restores." \
            -m "Part of the mayastor to ceph migration project."

          # Push
          git push

          echo "âœ… Changes committed and pushed"
          echo ""

          # Give Flux time to detect changes
          echo "ğŸ”„ Waiting for Flux to detect changes..."
          sleep 10

          # Trigger immediate reconciliation
          echo "   Triggering immediate Flux reconciliation..."
          flux reconcile source git flux-system --timeout 2m || true
          flux reconcile kustomization {{.APP}} --timeout 2m || true

          echo "âœ… Flux reconciliation triggered"
        else
          echo "ğŸ“‹ Manual commit mode - changes have been made to {{.KS_FILE}}"
          echo ""
          echo "   Review the changes with: git diff {{.KS_FILE}}"
          echo ""
          echo "   When ready, commit and push:"
          echo "   git add {{.KS_FILE}}"
          echo "   git commit -m 'chore({{.NS}}): migrate {{.APP}} PVC to {{.STORAGE_CLASS}}'"
          echo "   git push"
          echo ""

          read -p "Press ENTER once you've committed and pushed, or Ctrl+C to abort: "
          echo ""

          echo "ğŸ”„ Waiting for Flux to reconcile the changes..."
          sleep 10

          echo "   Triggering immediate Flux reconciliation..."
          flux reconcile source git flux-system --timeout 2m || true
          flux reconcile kustomization {{.APP}} --timeout 2m || true

          echo "âœ… Ready to proceed with migration"
        fi
        echo ""

  # Internal task: Scale down app
  _scale-down:
    internal: true
    cmds:
      - |
        echo "â¸ï¸  Scaling down {{.APP}}..."
        {{.KUBECTL}} --namespace {{.NS}} scale {{.CONTROLLER}}/{{.APP}} --replicas=0
        echo "   Waiting for pods to terminate..."
        {{.KUBECTL}} --namespace {{.NS}} wait pod --for=delete --selector="app.kubernetes.io/name={{.APP}}" --timeout=5m || true
        echo "âœ… App scaled down"

  # Internal task: Wait for PVC recreation
  _wait-for-pvc-recreation:
    internal: true
    cmds:
      - |
        echo "ğŸ”„ Triggering volsync restore with new storage class..."
        # Patch ReplicationDestination to trigger new restore with updated config
        {{.KUBECTL}} patch replicationdestination {{.APP}}-dst -n {{.NS}} --type merge -p "{\"spec\":{\"trigger\":{\"manual\":\"$(date +%s)\"}}}"

        echo "   Waiting for volsync restore job to start..."
        until {{.KUBECTL}} get job -n {{.NS}} 2>/dev/null | grep "volsync-dst-{{.APP}}-dst" &>/dev/null; do
          sleep 3
        done
        echo "   âœ… Restore job started"

        echo "   Waiting for restore job to complete..."
        # Wait for job with timeout, ignore if it completes quickly
        sleep 10

        echo "   Waiting for new snapshot to be created..."
        # Get the new snapshot name from ReplicationDestination status
        for i in {1..60}; do
          NEW_SNAPSHOT=$({{.KUBECTL}} get replicationdestination {{.APP}}-dst -n {{.NS}} -o jsonpath='{.status.latestImage.name}' 2>/dev/null)
          if [ -n "$NEW_SNAPSHOT" ]; then
            # Check if snapshot is ready
            SNAPSHOT_READY=$({{.KUBECTL}} get volumesnapshot "$NEW_SNAPSHOT" -n {{.NS}} -o jsonpath='{.status.readyToUse}' 2>/dev/null)
            if [ "$SNAPSHOT_READY" = "true" ]; then
              echo "   âœ… New snapshot ready: $NEW_SNAPSHOT"
              break
            fi
          fi
          sleep 5
        done

        echo "ğŸ—‘ï¸  Deleting volume snapshots that block PVC deletion..."
        # Delete snapshots that reference the PVC (they have finalizers preventing deletion)
        {{.KUBECTL}} get volumesnapshot -n {{.NS}} -o json | \
          jq -r --arg app "{{.APP}}" '.items[] | select(.spec.source.persistentVolumeClaimName==$app) | .metadata.name' | \
          xargs -r {{.KUBECTL}} delete volumesnapshot -n {{.NS}} || true

        echo "ğŸ”„ Deleting old PVC (spec is immutable)..."
        {{.KUBECTL}} --namespace {{.NS}} delete pvc {{.APP}} --wait=false

        echo "   Waiting for old PVC to be fully deleted..."
        for i in {1..30}; do
          if ! {{.KUBECTL}} --namespace {{.NS}} get pvc {{.APP}} &>/dev/null; then
            echo "   âœ… Old PVC deleted"
            break
          fi
          sleep 5
        done

        echo "ğŸ”„ Triggering Flux reconciliation to recreate PVC..."
        flux --namespace {{.NS}} reconcile kustomization {{.APP}} --with-source

        echo "   Waiting for new PVC to be created..."
        until {{.KUBECTL}} --namespace {{.NS}} get pvc {{.APP}} &>/dev/null; do
          sleep 3
        done

        echo "   Waiting for PVC to be Bound (volsync restoring from backup)..."
        {{.KUBECTL}} --namespace {{.NS}} wait pvc/{{.APP}} --for=jsonpath='{.status.phase}'=Bound --timeout=10m

        # Verify storage class
        ACTUAL_SC=$({{.KUBECTL}} --namespace {{.NS}} get pvc {{.APP}} -o jsonpath='{.spec.storageClassName}')
        if [ "$ACTUAL_SC" != "{{.STORAGE_CLASS}}" ]; then
          echo "âŒ PVC created with wrong storage class: $ACTUAL_SC (expected {{.STORAGE_CLASS}})"
          exit 1
        fi

        echo "âœ… PVC recreated with storage class: {{.STORAGE_CLASS}}"

  # Internal task: Scale up app
  _scale-up:
    internal: true
    cmds:
      - |
        echo "â–¶ï¸  Scaling up {{.APP}} to {{.REPLICAS}} replicas..."
        {{.KUBECTL}} --namespace {{.NS}} scale {{.CONTROLLER}}/{{.APP}} --replicas={{.REPLICAS}}
        echo "   Waiting for pods to be ready..."
        {{.KUBECTL}} --namespace {{.NS}} wait pod --for=condition=ready --selector="app.kubernetes.io/name={{.APP}}" --timeout=10m
        echo "âœ… App scaled up and running"

  # Internal task: Verify migration
  _verify-migration:
    internal: true
    cmds:
      - |
        echo "ğŸ” Verifying migration..."

        # Check PVC storage class
        ACTUAL_SC=$({{.KUBECTL}} --namespace {{.NS}} get pvc {{.APP}} -o jsonpath='{.spec.storageClassName}')
        echo "   PVC storage class: $ACTUAL_SC"

        # Check pod status
        POD_STATUS=$({{.KUBECTL}} --namespace {{.NS}} get pods -l app.kubernetes.io/name={{.APP}} -o jsonpath='{.items[0].status.phase}')
        echo "   Pod status: $POD_STATUS"

        if [ "$ACTUAL_SC" = "{{.STORAGE_CLASS}}" ] && [ "$POD_STATUS" = "Running" ]; then
          echo "âœ… Migration verified successfully"
        else
          echo "âš ï¸  Migration may have issues - please check manually"
        fi

  migration-status:
    desc: Show real-time migration status from cluster
    summary: |
      Displays current PVC distribution across storage classes and identifies stuck PVCs.
      Complements show-progress which shows config-based status.
    cmds:
      - |
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "Real-time Storage Migration Status (Cluster State):"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        # Count PVCs by storage class
        echo "PVCs by Storage Class:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        {{.KUBECTL}} get pvc -A -o json | jq -r '.items | group_by(.spec.storageClassName) | .[] | "\(.items | length) PVCs - \(.[0].spec.storageClassName // "default")"' | sort -rn

        echo ""
        echo "Stuck/Pending PVCs:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        STUCK=$({{.KUBECTL}} get pvc -A -o json | jq -r '.items[] | select(.status.phase != "Bound") | "\(.metadata.namespace)/\(.metadata.name) - \(.status.phase) - \(.spec.storageClassName)"')
        if [ -z "$STUCK" ]; then
          echo "âœ… No stuck PVCs"
        else
          echo "$STUCK"
        fi

        echo ""
        echo "Migration Summary:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        MAYASTOR=$({{.KUBECTL}} get pvc -A -o json | jq -r '[.items[] | select(.spec.storageClassName=="mayastor-single-replica")] | length')
        CEPHFS=$({{.KUBECTL}} get pvc -A -o json | jq -r '[.items[] | select(.spec.storageClassName=="cephfs-shared")] | length')
        TOTAL=$((MAYASTOR + CEPHFS))
        if [ $TOTAL -gt 0 ]; then
          PERCENT=$((CEPHFS * 100 / TOTAL))
          echo "  Mayastor PVCs remaining: $MAYASTOR"
          echo "  CephFS PVCs migrated:    $CEPHFS"
          echo "  Migration progress:      $PERCENT%"
        else
          echo "  No mayastor or cephfs PVCs found"
        fi
        echo ""

  cleanup-stuck-pvc:
    desc: Force cleanup a stuck PVC with finalizers [APP=required] [NS=required]
    summary: |
      Forcefully removes a PVC that is stuck due to finalizers or volume snapshots.

      Steps:
      1. Delete volume snapshots that reference the PVC
      2. Try normal PVC deletion with timeout
      3. Force delete with --grace-period=0
      4. Remove finalizers if still stuck
      5. Delete stuck PV if necessary

      Example:
        task storage:cleanup-stuck-pvc APP=huntarr NS=media
    vars:
      NS: '{{.NS | default "default"}}'
    requires:
      vars: [APP, NS]
    cmds:
      - |
        echo "ğŸ—‘ï¸  Cleaning up stuck PVC: {{.APP}} in namespace {{.NS}}"
        echo ""

        # Step 1: Delete volume snapshots
        echo "1. Deleting volume snapshots that reference this PVC..."
        SNAPSHOTS=$({{.KUBECTL}} get volumesnapshot -n {{.NS}} -o json 2>/dev/null | \
          jq -r --arg app "{{.APP}}" '.items[] | select(.spec.source.persistentVolumeClaimName==$app) | .metadata.name')

        if [ -n "$SNAPSHOTS" ]; then
          echo "$SNAPSHOTS" | while read snap; do
            echo "   Deleting snapshot: $snap"
            {{.KUBECTL}} delete volumesnapshot -n {{.NS}} "$snap" --wait=false || true
          done
        else
          echo "   No snapshots found"
        fi

        # Step 2: Try normal delete
        echo ""
        echo "2. Attempting normal PVC deletion..."
        {{.KUBECTL}} delete pvc {{.APP}} -n {{.NS}} --timeout=30s --wait=false 2>/dev/null || true
        sleep 5

        # Step 3: Check if still exists
        if {{.KUBECTL}} get pvc {{.APP}} -n {{.NS}} &>/dev/null; then
          echo ""
          echo "3. PVC still exists, force deleting..."
          {{.KUBECTL}} delete pvc {{.APP}} -n {{.NS}} --force --grace-period=0 2>/dev/null || true
          sleep 3

          # Step 4: Remove finalizers if still stuck
          if {{.KUBECTL}} get pvc {{.APP}} -n {{.NS}} &>/dev/null; then
            echo ""
            echo "4. Removing finalizers..."
            {{.KUBECTL}} patch pvc {{.APP}} -n {{.NS}} -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            sleep 2
          fi
        fi

        # Step 5: Verify deletion
        echo ""
        if {{.KUBECTL}} get pvc {{.APP}} -n {{.NS}} &>/dev/null; then
          echo "âš ï¸  PVC still exists - manual intervention may be required"
          echo "   Try: kubectl get pvc {{.APP}} -n {{.NS}} -o yaml"
        else
          echo "âœ… PVC successfully deleted"

          # Check for stuck PV
          PV=$({{.KUBECTL}} get pv -o json | jq -r --arg app "{{.APP}}" --arg ns "{{.NS}}" \
            '.items[] | select(.spec.claimRef.name==$app and .spec.claimRef.namespace==$ns) | .metadata.name')

          if [ -n "$PV" ]; then
            echo ""
            echo "âš ï¸  Associated PV still exists: $PV"
            echo "   To delete: kubectl delete pv $PV"
          fi
        fi
        echo ""

  list-static-mayastor:
    desc: List static PVCs that still use mayastor storage class
    summary: |
      Shows the 6 static cache PVCs with hardcoded mayastor-single-replica.
      These require manual migration using migrate-static-pvc task.
    cmds:
      - |
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "Static PVCs with Hardcoded Mayastor Storage Class:"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        STATIC_PVCS=(
          "media/plex:plex-cache:100Gi"
          "media/tautulli:tautulli-cache:15Gi"
          "media/overseerr:overseerr-cache:unknown"
          "ai/n8n:n8n-files:10Gi"
          "ai/open-webui:open-webui-pipelines:unknown"
          "observability/gatus:gatus:5Gi"
        )

        echo "These PVCs are safe to delete (they're caches, no volsync backups):"
        echo ""

        MIGRATED=0
        PENDING=0

        for pvc_info in "${STATIC_PVCS[@]}"; do
          IFS=':' read -r ns_app pvc_name size <<< "$pvc_info"
          ns=$(echo $ns_app | cut -d'/' -f1)
          app=$(echo $ns_app | cut -d'/' -f2)

          # Check current storage class
          CURRENT_SC=$({{.KUBECTL}} get pvc -n $ns $pvc_name -o jsonpath='{.spec.storageClassName}' 2>/dev/null || echo "NOT_FOUND")

          if [ "$CURRENT_SC" = "NOT_FOUND" ]; then
            echo "  â“ $ns/$pvc_name - PVC not found"
          elif [ "$CURRENT_SC" = "mayastor-single-replica" ]; then
            echo "  â³ $ns/$pvc_name ($size) - NEEDS MIGRATION"
            PENDING=$((PENDING + 1))
          elif [ "$CURRENT_SC" = "cephfs-shared" ]; then
            echo "  âœ… $ns/$pvc_name ($size) - Already migrated"
            MIGRATED=$((MIGRATED + 1))
          else
            echo "  â„¹ï¸  $ns/$pvc_name ($size) - Using: $CURRENT_SC"
          fi
        done

        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "Static PVC Status: $MIGRATED migrated, $PENDING pending"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        if [ $PENDING -gt 0 ]; then
          echo "To migrate a static PVC:"
          echo "  task storage:migrate-static-pvc APP=plex NS=media PVC_NAME=plex-cache"
        fi
        echo ""

  migrate-static-pvc:
    desc: Migrate a static cache PVC to new storage class [APP=required] [NS=required] [PVC_NAME=required] [STORAGE_CLASS=cephfs-shared]
    summary: |
      Migrates a static PVC (without volsync backup) to a new storage class.
      Safe for cache volumes - data will be lost but can be recreated.

      Steps:
      1. Update pvc.yaml manifest to use new storage class
      2. Commit changes (or wait for manual commit)
      3. Scale down app
      4. Delete old PVC (force if stuck)
      5. Reconcile to create new PVC
      6. Scale up app

      Examples:
        task storage:migrate-static-pvc APP=plex NS=media PVC_NAME=plex-cache
        task storage:migrate-static-pvc APP=gatus NS=observability PVC_NAME=gatus STORAGE_CLASS=cephfs-shared
    vars:
      NS: '{{.NS | default "default"}}'
      STORAGE_CLASS: '{{.STORAGE_CLASS | default "cephfs-shared"}}'
      PVC_FILE: '{{.ROOT_DIR}}/kubernetes/apps/{{.NS}}/{{.APP}}/app/pvc.yaml'
      CONTROLLER:
        sh: '{{.KUBECTL}} --namespace {{.NS}} get deployment {{.APP}} &>/dev/null && echo deployment || ({{.KUBECTL}} --namespace {{.NS}} get statefulset {{.APP}} &>/dev/null && echo statefulset || echo cronjob)'
      ORIGINAL_REPLICAS:
        sh: '{{.KUBECTL}} --namespace {{.NS}} get {{.CONTROLLER}}/{{.APP}} -o jsonpath="{.spec.replicas}" 2>/dev/null || echo "1"'
    requires:
      vars: [APP, NS, PVC_NAME]
    preconditions:
      - sh: '[ -f "{{.PVC_FILE}}" ]'
        msg: 'PVC file not found at {{.PVC_FILE}}'
      - sh: '{{.KUBECTL}} --namespace {{.NS}} get pvc {{.PVC_NAME}} &>/dev/null'
        msg: 'PVC {{.PVC_NAME}} not found in namespace {{.NS}}'
    cmds:
      - |
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "Migrating Static PVC: {{.PVC_NAME}} ({{.NS}}/{{.APP}})"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""

        # Step 1: Update PVC manifest
        echo "1. Updating PVC manifest..."
        if ! command -v yq &>/dev/null; then
          echo "âŒ yq not found. Please install yq or manually edit {{.PVC_FILE}}"
          echo "   Change: storageClassName: mayastor-single-replica"
          echo "       To: storageClassName: {{.STORAGE_CLASS}}"
          exit 1
        fi

        # Use sed for simple replacement (more reliable than yq for multi-doc)
        sed -i 's/storageClassName: mayastor-single-replica/storageClassName: {{.STORAGE_CLASS}}/g' {{.PVC_FILE}}
        echo "   Updated {{.PVC_FILE}}"
        echo ""

        # Step 2: Commit or wait
        echo "2. Commit changes..."
        git add {{.PVC_FILE}}

        read -p "   Commit and push now? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
          git commit -m "chore({{.NS}}): migrate {{.PVC_NAME}} to {{.STORAGE_CLASS}}" \
            -m "Migrating static cache PVC from mayastor to {{.STORAGE_CLASS}}." \
            -m "Safe to recreate - this is a cache volume."
          git push
          echo "   âœ… Changes committed and pushed"

          # Trigger Flux reconciliation
          sleep 5
          flux reconcile source git flux-system --timeout 1m || true
          flux reconcile kustomization {{.APP}} --timeout 1m || true
        else
          echo "   Skipped - please commit manually before continuing"
          read -p "   Press ENTER when ready to continue: "
        fi
        echo ""

        # Step 3: Scale down
        echo "3. Scaling down {{.APP}}..."
        if [ "{{.CONTROLLER}}" = "cronjob" ]; then
          {{.KUBECTL}} patch cronjob {{.APP}} -n {{.NS}} -p '{"spec":{"suspend":true}}'
        else
          {{.KUBECTL}} --namespace {{.NS}} scale {{.CONTROLLER}}/{{.APP}} --replicas=0
          {{.KUBECTL}} --namespace {{.NS}} wait pod --for=delete --selector="app.kubernetes.io/name={{.APP}}" --timeout=5m || true
        fi
        echo "   âœ… App scaled down"
        echo ""

        # Step 4: Delete PVC
        echo "4. Deleting old PVC..."
        task storage:cleanup-stuck-pvc APP={{.PVC_NAME}} NS={{.NS}}

        # Step 5: Reconcile
        echo "5. Reconciling to create new PVC..."
        flux reconcile kustomization {{.APP}} --timeout 2m

        echo "   Waiting for new PVC to be created..."
        until {{.KUBECTL}} --namespace {{.NS}} get pvc {{.PVC_NAME}} &>/dev/null; do
          sleep 3
        done

        echo "   Waiting for PVC to be Bound..."
        {{.KUBECTL}} --namespace {{.NS}} wait pvc/{{.PVC_NAME}} --for=jsonpath='{.status.phase}'=Bound --timeout=5m

        ACTUAL_SC=$({{.KUBECTL}} --namespace {{.NS}} get pvc {{.PVC_NAME}} -o jsonpath='{.spec.storageClassName}')
        if [ "$ACTUAL_SC" != "{{.STORAGE_CLASS}}" ]; then
          echo "   âŒ PVC created with wrong storage class: $ACTUAL_SC"
          exit 1
        fi
        echo "   âœ… PVC recreated with: {{.STORAGE_CLASS}}"
        echo ""

        # Step 6: Scale up
        echo "6. Scaling up {{.APP}}..."
        if [ "{{.CONTROLLER}}" = "cronjob" ]; then
          {{.KUBECTL}} patch cronjob {{.APP}} -n {{.NS}} -p '{"spec":{"suspend":false}}'
          echo "   âœ… CronJob resumed"
        else
          {{.KUBECTL}} --namespace {{.NS}} scale {{.CONTROLLER}}/{{.APP}} --replicas={{.ORIGINAL_REPLICAS}}
          {{.KUBECTL}} --namespace {{.NS}} wait pod --for=condition=ready --selector="app.kubernetes.io/name={{.APP}}" --timeout=10m
          echo "   âœ… App scaled up and running"
        fi
        echo ""

        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "âœ… Migration complete for {{.PVC_NAME}}"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
