---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # Routes requests to local Ollama models with cloud fallback

    model_list:
      # ============================================
      # LOCAL MODELS (Ollama on P100 GPUs)
      # ============================================

      # Fast general model - Qwen 14B
      - model_name: local/qwen-14b
        litellm_params:
          model: ollama/qwen2.5:14b
          api_base: http://ollama.ai.svc.cluster.local:11434
        model_info:
          description: "Local Qwen2.5 14B for general tasks"
          max_tokens: 8192

      # Fast small model - Qwen 7B
      - model_name: local/qwen-7b
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama.ai.svc.cluster.local:11434
        model_info:
          description: "Local Qwen2.5 7B for quick tasks"
          max_tokens: 8192

      # Coding model
      - model_name: local/qwen-coder
        litellm_params:
          model: ollama/qwen2.5-coder:14b
          api_base: http://ollama.ai.svc.cluster.local:11434
        model_info:
          description: "Local coding model"
          max_tokens: 8192

      # ============================================
      # CLOUD FALLBACK MODELS
      # ============================================

      # Claude for complex tasks
      - model_name: claude-sonnet
        litellm_params:
          model: anthropic/claude-sonnet-4-20250514
          api_key: os.environ/ANTHROPIC_API_KEY
        model_info:
          description: "Claude Sonnet 4 for complex reasoning"

      - model_name: claude-haiku
        litellm_params:
          model: anthropic/claude-3-5-haiku-20241022
          api_key: os.environ/ANTHROPIC_API_KEY
        model_info:
          description: "Claude Haiku for fast cloud responses"

      # OpenAI for compatibility
      - model_name: gpt-4o
        litellm_params:
          model: openai/gpt-4o
          api_key: os.environ/OPENAI_API_KEY
        model_info:
          description: "OpenAI GPT-4o"

      - model_name: gpt-4o-mini
        litellm_params:
          model: openai/gpt-4o-mini
          api_key: os.environ/OPENAI_API_KEY
        model_info:
          description: "OpenAI GPT-4o Mini for cost efficiency"

      # ============================================
      # ALIASED ROUTING (catch OpenAI model names)
      # ============================================

      # gpt-3.5-turbo requests → local fast model
      - model_name: gpt-3.5-turbo
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama.ai.svc.cluster.local:11434
        model_info:
          description: "Alias → local Qwen 7B"

      # gpt-4 requests → local reasoning model
      - model_name: gpt-4
        litellm_params:
          model: ollama/qwen2.5:14b
          api_base: http://ollama.ai.svc.cluster.local:11434
        model_info:
          description: "Alias → local Qwen 14B"

    # Router settings
    router_settings:
      routing_strategy: least-busy
      enable_pre_call_checks: true
      num_retries: 2
      timeout: 120
      retry_after: 5
      fallbacks:
        - gpt-4: [claude-sonnet, gpt-4o]
        - gpt-3.5-turbo: [claude-haiku, gpt-4o-mini]
        - local/qwen-14b: [claude-sonnet]
        - local/qwen-7b: [claude-haiku]

    # General settings
    general_settings:
      master_key: os.environ/LITELLM_MASTER_KEY
      database_url: os.environ/DATABASE_URL

    # Logging
    litellm_settings:
      success_callback: ["prometheus"]
      failure_callback: ["prometheus"]
      service_callback: ["prometheus"]
      cache: true
      cache_params:
        type: redis
        host: dragonfly.database.svc.cluster.local
        port: 6379
