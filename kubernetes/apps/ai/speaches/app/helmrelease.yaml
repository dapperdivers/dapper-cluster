---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app speaches
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: volsync
      namespace: storage
    - name: nvidia-device-plugin
      namespace: kube-system
  values:
    controllers:
      speaches:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/speaches-ai/speaches
              tag: latest-cuda
            env:
              TZ: ${TIME_ZONE}
              # Use int8 quantization (optimized for P100/Pascal GPUs)
              WHISPER__COMPUTE_TYPE: int8
              WHISPER__INFERENCE_DEVICE: cuda
              # Keep models loaded in memory (no unload delay)
              STT_MODEL_TTL: "-1"
              # Preload the large-v3-turbo model at startup
              PRELOAD_MODELS: '["deepdml/faster-whisper-large-v3-turbo-ct2"]'
              # Disable UI to reduce overhead
              ENABLE_UI: "false"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: &port 8000
                  initialDelaySeconds: 30
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: *port
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3
            resources:
              requests:
                cpu: 100m
                memory: 2Gi
                nvidia.com/gpu: 1
              limits:
                memory: 8Gi
                nvidia.com/gpu: 1
    defaultPodOptions:
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.feature.node.kubernetes.io/gpu: "true"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.feature.node.kubernetes.io/gpu
                    operator: In
                    values:
                      - "true"
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: *app
        ports:
          http:
            port: *port
    ingress:
      app:
        annotations:
          external-dns.alpha.kubernetes.io/target: internal.${SECRET_DOMAIN}
        className: internal
        hosts:
          - host: &host speaches.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host
            secretName: ${SECRET_DOMAIN/./-}-tls
    persistence:
      data:
        existingClaim: *app
        globalMounts:
          - path: /home/ubuntu/.cache/huggingface/hub
      cache:
        type: emptyDir
        globalMounts:
          - path: /.cache
