---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  maxHistory: 3
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3

  values:
    controllers:
      ollama:
        annotations:
          reloader.stakater.com/auto: "true"
        replicas: 1

        containers:
          app:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.15.4
            env:
              TZ: ${TIME_ZONE}
              # Store models on PVC
              OLLAMA_MODELS: /models
              # Listen on all interfaces
              OLLAMA_HOST: 0.0.0.0:11434
              # Keep models loaded longer (single user)
              OLLAMA_KEEP_ALIVE: 30m
              # Enable flash attention where supported
              OLLAMA_FLASH_ATTENTION: "1"
              # Number of parallel requests
              OLLAMA_NUM_PARALLEL: "2"
            ports:
              - name: http
                containerPort: &port 11434
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/tags
                    port: *port
                  initialDelaySeconds: 10
                  periodSeconds: 30
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/tags
                    port: *port
                  initialDelaySeconds: 10
                  periodSeconds: 10
            resources:
              requests:
                cpu: 2
                memory: 8Gi
                nvidia.com/gpu: 2
              limits:
                cpu: 8
                memory: 24Gi
                nvidia.com/gpu: 2

    defaultPodOptions:
      nodeSelector:
        nvidia.feature.node.kubernetes.io/gpu: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

    service:
      app:
        controller: ollama
        ports:
          http:
            port: *port

    ingress:
      app:
        className: internal
        annotations:
          external-dns.alpha.kubernetes.io/target: internal.${SECRET_DOMAIN}
        hosts:
          - host: &host ollama.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host
            secretName: ${SECRET_DOMAIN/./-}-tls

    persistence:
      models:
        existingClaim: ollama-models
        globalMounts:
          - path: /models
