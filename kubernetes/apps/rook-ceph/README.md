# Rook Ceph External Cluster

This directory contains the configuration for connecting Kubernetes to the external Ceph cluster running on Proxmox hosts.

## Structure

```
rook-ceph/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ kustomization.yaml                 # Main kustomization
â”œâ”€â”€ rook-ceph-operator/               # Rook operator deployment
â”‚   â”œâ”€â”€ ks.yaml                       # Flux Kustomization
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â”œâ”€â”€ namespace.yaml            # rook-ceph namespace
â”‚       â””â”€â”€ helmrelease.yaml          # Rook operator Helm release
â””â”€â”€ rook-ceph-cluster/                # External cluster configuration
    â”œâ”€â”€ ks.yaml                       # Flux Kustomization
    â””â”€â”€ app/
        â”œâ”€â”€ kustomization.yaml
        â”œâ”€â”€ configmap.yaml            # Mon endpoints (NEEDS CONFIGURATION)
        â”œâ”€â”€ secret.sops.yaml          # Ceph credentials (NEEDS CONFIGURATION)
        â”œâ”€â”€ cluster-external.yaml     # CephCluster CR
        â””â”€â”€ storageclasses.yaml       # Storage classes

```

## Project Phases

### Phase 1: Initial Connection & Mayastor Migration âœ… COMPLETED
**Goal**: Connect to external Ceph, migrate workloads from OpenEBS Mayastor to CephFS

**Status**: âœ… PRODUCTION - Migration Complete
- âœ… ConfigMap configured (mon endpoints, FSID)
- âœ… CephFS storage class configured (default storage class)
- âœ… Rook operator configured (CephFS driver enabled)
- âœ… Ceph credentials configured and working
- âœ… Static PVs deployed (media, minio, paperless)
- âœ… VolSync migrated to CephFS backend
- âœ… Workloads migrated from Mayastor to CephFS

**Current Setup**:
- CephFS is the primary storage solution
- RBD driver disabled (Phase 2 future work)
- RBD storage classes commented out (Phase 2)

### Phase 2: Optimized Pool Creation (Future)
After hardware migration from Mayastor:
1. Create RBD pools (ssd-db, rook-pvc-pool, media-bulk)
2. Enable RBD driver in operator
3. Uncomment RBD storage classes
4. Update client.kubernetes permissions
5. Migrate workloads to optimized pools

## Storage Classes

### Phase 1 (Active)
| Class Name | Type | Use Case |
|------------|------|----------|
| `cephfs-shared` | CephFS (RWX) | All workloads during Mayastor migration |

### Phase 2 (Future - After Pool Creation)
| Class Name | Pool | Type | Use Case |
|------------|------|------|----------|
| `ceph-ssd-critical` | ssd-db | RBD | Databases, etcd, critical apps |
| `ceph-rbd` | rook-pvc-pool | RBD | General application storage |
| `ceph-bulk` | media-bulk | RBD (EC) | Media, backups, large files |

## Network

- **Ceph Public Network**: 10.150.0.0/24 - Client connections
- **Ceph Storage Network**: 10.200.0.0/24 - OSD replication (not used by K8s)

## Documentation

### Phase 1 (Current)
- **ðŸ“‹ Quick Start**: `docs/rook-phase1-quickstart.md` - **START HERE** for deployment
- **ðŸ“– Deployment Steps**: `docs/rook-deployment-steps.md` - Detailed step-by-step guide
- **ðŸ”§ Complete Setup**: `docs/rook-external-ceph-setup.md` - Full documentation with troubleshooting

### Phase 2 (Future)
- **ðŸš€ Migration Guide**: `docs/rook-phase2-migration.md` - Pool optimization and RBD migration

## Operations - Phase 1 (Production)

### Verify Cluster Health
```bash
# Check CephCluster resource
kubectl -n rook-ceph get cephcluster

# Check Ceph health via toolbox
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status

# Check CephFS status
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph fs status

# List storage classes
kubectl get sc
```

### Monitor Storage
```bash
# Check Ceph capacity
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph df

# Check OSD status
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd tree

# Check MDS performance
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph fs status cephfs
```

### Common Operations
```bash
# Expand a PVC
kubectl patch pvc <pvc-name> -n <namespace> -p '{"spec":{"resources":{"requests":{"storage":"20Gi"}}}}'

# Check all PVCs using CephFS
kubectl get pvc -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,STORAGECLASS:.spec.storageClassName | grep cephfs
```
