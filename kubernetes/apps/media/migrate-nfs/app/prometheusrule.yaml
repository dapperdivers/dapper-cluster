---
# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/monitoring.coreos.com/prometheusrule_v1.json
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: migration-alerts
  namespace: media
spec:
  groups:
    - name: migration.rules
      rules:
        # Alert when migration job fails
        - alert: MigrationJobFailed
          annotations:
            summary: "Migration job {{ $labels.job_name }} failed"
            description: "Migration job {{ $labels.job_name }} in namespace {{ $labels.namespace }} has failed. Check logs for errors."
          expr: |
            kube_job_status_failed{namespace="media", job_name=~"migrate.*"} > 0
          for: 2m
          labels:
            severity: warning
            component: migration

        # Alert when migration job is running longer than expected
        - alert: MigrationJobRunningTooLong
          annotations:
            summary: "Migration job {{ $labels.job_name }} running for over 48 hours"
            description: "Migration job {{ $labels.job_name }} has been active for more than 48 hours. This may indicate it's stuck."
          expr: |
            (time() - kube_job_status_start_time{namespace="media", job_name=~"migrate.*"}) > 172800
            and
            kube_job_status_active{namespace="media", job_name=~"migrate.*"} > 0
          for: 10m
          labels:
            severity: warning
            component: migration

        # Alert when no migration pods are running but job is active
        - alert: MigrationJobNoActivePods
          annotations:
            summary: "Migration job {{ $labels.job_name }} has no active pods"
            description: "Migration job {{ $labels.job_name }} is marked as active but has no running pods. Job may be stuck."
          expr: |
            kube_job_status_active{namespace="media", job_name=~"migrate.*"} > 0
            unless
            kube_pod_status_phase{namespace="media", pod=~"migrate.*", phase="Running"} > 0
          for: 15m
          labels:
            severity: warning
            component: migration

        # Storage capacity alerts
        - alert: MigrationDestinationStorageLow
          annotations:
            summary: "Destination storage (CephFS) running low during migration"
            description: "CephFS storage is at {{ $value | humanizePercentage }} capacity. Migration may fail due to insufficient space."
          expr: |
            (
              kubelet_volume_stats_used_bytes{namespace="media", persistentvolumeclaim="media-cephfs-pvc"}
              /
              kubelet_volume_stats_capacity_bytes{namespace="media", persistentvolumeclaim="media-cephfs-pvc"}
            ) * 100 > 85
            and
            kube_job_status_active{namespace="media", job_name=~"migrate.*"} > 0
          for: 5m
          labels:
            severity: critical
            component: storage

        # CephFS write issues during migration
        - alert: CephFSWritesDuringMigration
          annotations:
            summary: "CephFS writes degraded during migration"
            description: "CephFS write operations have stopped or significantly reduced during active migration."
          expr: |
            rate(ceph_pool_wr{name="cephfs-data"}[5m]) < 1
            and
            kube_job_status_active{namespace="media", job_name=~"migrate.*"} > 0
          for: 10m
          labels:
            severity: warning
            component: storage

        # Unraid server health alerts
        - alert: UnraidServerDown
          annotations:
            summary: "Unraid NFS server {{ $labels.instance }} is down"
            description: "Unraid server {{ $labels.instance }} has been unreachable for over 5 minutes. Migration may be affected."
          expr: |
            up{job="node-exporter", instance=~"tower.*"} == 0
          for: 5m
          labels:
            severity: critical
            component: unraid

        - alert: UnraidHighCPU
          annotations:
            summary: "High CPU usage on Unraid server {{ $labels.instance }}"
            description: "Unraid server {{ $labels.instance }} CPU usage is at {{ $value | humanizePercentage }}. This may impact migration performance."
          expr: |
            100 - (avg by(instance) (irate(node_cpu_seconds_total{job="node-exporter",instance=~"tower.*",mode="idle"}[5m])) * 100) > 80
          for: 15m
          labels:
            severity: warning
            component: unraid

        - alert: UnraidHighMemory
          annotations:
            summary: "High memory usage on Unraid server {{ $labels.instance }}"
            description: "Unraid server {{ $labels.instance }} memory usage is at {{ $value | humanizePercentage }}. This may cause performance issues."
          expr: |
            ((node_memory_MemTotal_bytes{job="node-exporter",instance=~"tower.*"} - node_memory_MemAvailable_bytes{job="node-exporter",instance=~"tower.*"}) / node_memory_MemTotal_bytes{job="node-exporter",instance=~"tower.*"}) * 100 > 90
          for: 15m
          labels:
            severity: warning
            component: unraid

        - alert: UnraidNetworkErrors
          annotations:
            summary: "Network errors detected on Unraid server {{ $labels.instance }}"
            description: "Unraid server {{ $labels.instance }} is experiencing network errors. This may impact NFS performance during migration."
          expr: |
            rate(node_network_receive_errs_total{job="node-exporter",instance=~"tower.*",device!~"lo|veth.*|br.*|docker.*"}[5m]) > 10
            or
            rate(node_network_transmit_errs_total{job="node-exporter",instance=~"tower.*",device!~"lo|veth.*|br.*|docker.*"}[5m]) > 10
          for: 10m
          labels:
            severity: warning
            component: unraid

        - alert: UnraidNFSDegraded
          annotations:
            summary: "NFS server performance degraded on {{ $labels.instance }}"
            description: "NFS server on {{ $labels.instance }} is experiencing reduced throughput during migration."
          expr: |
            rate(node_nfsd_disk_bytes_read_total{job="node-exporter",instance=~"tower.*"}[5m]) < 1048576
            and
            kube_job_status_active{namespace="media", job_name=~"migrate.*"} > 0
          for: 15m
          labels:
            severity: warning
            component: unraid
