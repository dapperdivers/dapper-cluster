---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app tdarr-node-cpu
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: volsync
      namespace: storage
  values:
    controllers:
      tdarr-node-cpu:
        annotations:
          reloader.stakater.com/auto: "true"
        # Replicas managed by HPA - don't set here
        strategy: RollingUpdate
        pod:
          priorityClassName: tdarr-priority  # Use lower priority for batch processing
          # Run on large nodes with available CPU/RAM
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app.kubernetes.io/name
                          operator: In
                          values:
                            - tdarr-node-cpu
                    topologyKey: kubernetes.io/hostname

        containers:
          app:
            image:
              repository: ghcr.io/haveagitgat/tdarr_node
              tag: 2.48.01@sha256:19bd23303defefa77aca01a8bc93abb6dcab685137f74ae0885071a709f4fcd8
            envFrom:
              - configMapRef:
                  name: tdarr-config
                  optional: true
            env:
              TZ: ${TIME_ZONE}
              PGID: 140
              PUID: 1000
              # Tdarr Server connection
              serverIP: tdarr.media
              serverPort: "8266"
              # Node identification
              nodeName: "CPU-$(POD_NAME)"
              nodeID: "CPU-$(POD_NAME)"
              # Node configuration
              inContainer: "true"
              ffmpegVersion: "7"
              # CPU-only processing
              nodeHardwareType: cpu
              # Performance settings
              nodeIP: "0.0.0.0"
              nodePort: "8267"

              # Worker configuration for CPU-only nodes
              transcodegpuWorkers: "0"  # No GPU workers on CPU nodes
              transcodecpuWorkers: "2"  # Use 2 CPU workers for transcoding
              healthcheckgpuWorkers: "0"  # No GPU health checks
              healthcheckcpuWorkers: "1"  # Use 1 CPU worker for health checks
              pollInterval: "2000"  # Poll every 2 seconds
              startPaused: "false"  # Start processing immediately
              maxLogSizeMB: "10"  # Log size limit
              logLevel: "INFO"
              priority: "-1"

              POD_NAME:
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
            resources:
              requests:
                cpu: 2000m  # 2 CPU cores
                memory: 8Gi
              limits:
                cpu: 4000m  # Allow bursting to 4 cores
                memory: 16Gi
    service:
      app:
        controller: tdarr-node-cpu
        ports:
          server:
            enabled: true
            protocol: TCP
            port: 8266

    persistence:
      safe:
        existingClaim: media-cephfs-pvc
        globalMounts:
          - path: /safe
      transcode:
        type: emptyDir
        medium: Memory
        sizeLimit: 30Gi  # 30GB RAM disk for CPU nodes
        globalMounts:
          - path: /temp
