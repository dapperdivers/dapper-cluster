---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app tdarr-node
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: volsync
      namespace: storage
    - name: nvidia-device-plugin
      namespace: kube-system
  values:
    controllers:
      tdarr-node:
        annotations:
          reloader.stakater.com/auto: "true"
        # Replicas managed by HPA - don't set here
        # Replicas managed by HPA - don't set here
        strategy: RollingUpdate
        pod:
          nodeSelector:
            nvidia.feature.node.kubernetes.io/gpu: "true"
        containers:
          app: &node
            image:
              repository: ghcr.io/haveagitgat/tdarr_node
              tag: 2.46.01@sha256:1ecdd628d424a98773e61a6fd85ec1533da0259baa1aa8c806960cfe2b56aa1b
            envFrom:
              - configMapRef:
                  name: tdarr-config
                  optional: true
            env:
              TZ: ${TIME_ZONE}
              PGID: 140
              PUID: 1000
              # Tdarr Server connection
              serverIP: tdarr.media
              serverPort: "8266"
              # GPU configuration
              NVIDIA_DRIVER_CAPABILITIES: all
              NVIDIA_VISIBLE_DEVICES: all
              # Node identification
              nodeName: "GPU-$(POD_NAME)"
              nodeID: "GPU-$(POD_NAME)"
              # Node configuration
              inContainer: "true"
              ffmpegVersion: "7"
              # Performance settings
              nodeIP: "0.0.0.0"
              nodePort: "8267"

              # Worker configuration for GPU nodes
              transcodegpuWorkers: "1"  # Use GPU for transcoding
              transcodecpuWorkers: "0"  # Don't use CPU for transcoding on GPU nodes
              healthcheckgpuWorkers: "1"  # Use GPU for health checks
              healthcheckcpuWorkers: "0"  # Don't use CPU for health checks on GPU nodes
              pollInterval: "2000"  # Poll every 2 seconds
              startPaused: "false"  # Start processing immediately
              maxLogSizeMB: "10"  # Log size limit
              logLevel: "INFO"
              priority: "-1"

              POD_NAME:
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
            resources:
              requests:
                cpu: 1000m
                memory: 10Gi
                nvidia.com/gpu: 4  # Request 1 full GPU (4 slices)
              limits:
                memory: 20Gi
                nvidia.com/gpu: 4  # Limit to 1 full GPU
    defaultPodOptions:
      runtimeClassName: nvidia
      priorityClassName: tdarr-priority  # Use lower priority for batch processing
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.feature.node.kubernetes.io/gpu
                  operator: In
                  values:
                    - "true"
    service:
      app:
        controller: tdarr-node
        ports:
          server:
            enabled: true
            protocol: TCP
            port: 8266

    persistence:
      configs:
        type: configMap
        name: tdarr-node-configs
        globalMounts:
          - path: /app/configs
            readOnly: false
      safe:
        existingClaim: media-vault-pvc
        globalMounts:
          - path: /safe
      transcode:
        type: emptyDir
        medium: Memory
        sizeLimit: 40Gi
        globalMounts:
          - path: /temp
