# Rook Ceph Orphaned Volumes & Snapshot Deadlock - Complete Resolution

**Date:** October 30 - November 3, 2025
**Severity:** Critical (98% snapshot failure, 10+ applications stuck)
**Status:** ‚úÖ ROOT CAUSE IDENTIFIED - Resolution in progress
**Duration:** 5 days investigation

---

## Executive Summary

### The Real Problem

**247 orphaned RBD volumes (86% of total Ceph volumes!)** were accumulating in Ceph due to:
1. Missing `discard` mount option in StorageClass (volumes not cleaned up when PVCs deleted)
2. 108 volumes with **stale Ceph watchers** preventing cleanup
3. CSI driver attempting to unmap these volumes on every restart
4. Unmap operations failing with `rbd: unknown unmap option 'tcmu'` error
5. Failed unmap operations **blocking ALL snapshot creation** (62 snapshots stuck)

**Impact:**
- ‚úÖ CephFS operations: Working fine
- ‚ùå RBD snapshots: 98% failure rate
- ‚ùå VolSync backups/restores: Completely blocked
- ‚ùå 10+ applications: Stuck for 30+ hours

### Root Cause Timeline

```
Oct 21, 2025: Last successful RBD snapshots (20+)
Oct 29, 2025: Snapshot operations begin failing (70/71 failures)
Oct 30, 2025: Investigation begins - suspected CSI deadlock
Oct 31, 2025: Discovered 247 orphaned volumes (86% of pool!)
Nov 03, 2025: Found 108 volumes with stale watchers blocking CSI unmap
            ‚Üí Blocking ALL snapshot operations
```

---

## Technical Root Cause

### 1. Orphaned Volume Accumulation

**Missing Prevention:**
```yaml
# StorageClass was missing discard mount option
mountOptions:
  - discard  # ‚Üê MISSING: Prevents space reclamation
```

**Result:** When PVCs were deleted, Kubernetes removed the PV metadata but Ceph RBD volumes remained orphaned in the pool.

**Scale:** 247 orphaned volumes out of 285 total (86% orphaned rate!)

### 2. Stale Watchers Preventing Cleanup

**The Deadlock Chain:**

1. **Orphaned volumes created** (247 total from missing `discard` option)
2. **Bulk cleanup attempted** (139 succeeded, 108 failed)
3. **108 volumes have stale watchers:**
   ```
   Watchers:
     watcher=10.150.0.15:0/2183527974 client.6306334 cookie=18446462598732840970
   ```
4. **CSI attempts unmap on restart:**
   ```
   rbd unmap rook-pvc-pool/csi-vol-XXX --device-type krbd --options noudev --options force,tcmu
   ```
5. **Unmap fails with:**
   ```
   rbd: unknown unmap option 'tcmu'
   rbd: unmap failed (exit status 22)
   ```
6. **Failed unmap operations BLOCK snapshot controller** from processing ANY snapshots
7. **62 VolumeSnapshots stuck** waiting for CSI to clear unmap queue

### 3. Why Snapshots Are Blocked

The CSI snapshotter is attempting to create snapshots, but the CSI RBD plugin is stuck trying to unmap 108 orphaned volumes with stale watchers. Until these unmap operations complete (or are cleared), NO snapshots can proceed.

**Evidence:**
```bash
# CSI snapshotter attempting snapshots:
I1103 19:51:49.464699 createSnapshotWrapper: Creating snapshot for content snapcontent-04863b6c...

# CSI RBD plugin failing to unmap orphaned volumes:
E1103 19:53:35.065655 error unmapping volume csi-vol-7a40f358-308e-427c-b343-60c3a8e6438c
rbd: unknown unmap option 'tcmu'

# Result: 62 snapshots stuck at readyToUse: false
```

---

## Resolution Steps

### Phase 1: Initial Cleanup ‚úÖ COMPLETED

**Cleaned 139 orphaned volumes:**
```bash
# Deleted 139/247 volumes (56% success rate)
# 108 volumes failed due to stale watchers
# Results logged in:
#   /tmp/cleanup-success.log (139 volumes)
#   /tmp/cleanup-failed.log (108 volumes)
```

### Phase 2: Force Cleanup Stale Watchers üîÑ IN PROGRESS

**Script created:** `/tmp/force-cleanup-watched-volumes.sh`

**Process:**
1. For each of 108 orphaned volumes with watchers
2. Identify stale Ceph clients (e.g., `client.6306334`)
3. Blacklist them in Ceph: `ceph osd blacklist add <client>`
4. Force-delete the orphaned volume
5. Restart CSI plugins to clear unmap queue

**Expected outcome:**
- ‚úÖ All 108 orphaned volumes removed
- ‚úÖ CSI unmap queue cleared
- ‚úÖ 62 pending snapshots can complete
- ‚úÖ gatus and vaultwarden PVCs provision successfully

### Phase 3: Prevention ‚è≥ PENDING COMMIT

**Added to StorageClass** (not yet committed per user request):
```yaml
# kubernetes/apps/rook-ceph/rook-ceph-cluster/app/storageclasses.yaml
provisioner: rook-ceph.rbd.csi.ceph.com
mountOptions:
  - discard  # ‚Üê Prevents future orphaned volumes
```

**Note:** Only affects NEW PVCs created after this change.

---

## What We Learned

### Red Herrings (Things That Weren't the Problem)

1. ‚ùå **Rook version mismatch** - Rook version doesn't need to match external Ceph
2. ‚ùå **CSI snapshotter v8.x issues** - Not the root cause, just exposed the problem
3. ‚ùå **Ceph cluster performance** - Cluster healthy at 435 ops/s
4. ‚ùå **unmapOptions configuration** - Valid config, not the issue
5. ‚ùå **CSI operation locks in Ceph omap** - Symptom, not cause

### Actual Root Causes

1. ‚úÖ **Missing `discard` mount option** - Caused orphaned volume accumulation
2. ‚úÖ **Stale Ceph watchers** - Prevented cleanup of orphaned volumes
3. ‚úÖ **CSI unmap failures** - Blocked snapshot controller queue
4. ‚úÖ **Lack of automated cleanup** - Rook/Ceph CSI doesn't auto-clean orphaned volumes (known limitation)

---

## Verification Commands

### Check Orphaned Volumes
```bash
# Get current K8s PVs
kubectl get pv -o json | jq -r '.items[].spec.csi.volumeHandle' | \
  grep "^0001-" | cut -d'-' -f5- | sort > /tmp/k8s-volumes.txt

# Get Ceph RBD volumes
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  rbd ls rook-pvc-pool | sort > /tmp/ceph-volumes.txt

# Find orphans (in Ceph but not in K8s)
comm -13 /tmp/k8s-volumes.txt /tmp/ceph-volumes.txt
```

### Check for Stale Watchers
```bash
# Check specific volume for watchers
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  rbd status rook-pvc-pool/csi-vol-<UUID>

# Blacklist a stale client
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
  ceph osd blacklist add <client-ip:port>
```

### Check CSI Status
```bash
# Check for unmap errors
kubectl logs -n rook-ceph daemonset/csi-rbdplugin -c csi-rbdplugin --tail=100 | \
  grep "error unmapping"

# Check snapshot controller status
kubectl logs -n rook-ceph deployment/csi-rbdplugin-provisioner -c csi-snapshotter --tail=50

# Count stuck snapshots
kubectl get volumesnapshot -A | grep "false" | wc -l
```

### Check Ceph Health
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd pool stats rook-pvc-pool
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- rados -p rook-pvc-pool ls | grep csi-vol | wc -l
```

---

## Impact Analysis

### Before Cleanup
- **Total Ceph volumes:** 285
- **K8s PVs:** 38
- **Orphaned volumes:** 247 (86%)
- **Ceph usage:** 222 TiB allocated, significant waste

### After Initial Cleanup
- **Total Ceph volumes:** 146 (48% reduction)
- **K8s PVs:** 38 (unchanged)
- **Orphaned volumes:** 108 (still problematic - have watchers)
- **Successfully deleted:** 139 volumes

### Expected After Full Cleanup
- **Total Ceph volumes:** ~38 (matching K8s PVs)
- **Orphaned volumes:** 0
- **Snapshot operations:** Fully functional
- **Space reclaimed:** Significant

---

## Affected Systems

### Infrastructure Versions
- **Rook:** v1.18.6 (operator)
- **Ceph-CSI:** v3.15.0
- **External Ceph:** v18.2.7 (reef)
- **Cluster:** 29 OSDs, 222 TiB total capacity
- **Health:** HEALTH_OK throughout incident

### Applications Affected
**Critical (stuck for 30+ hours):**
- gatus (observability)
- vaultwarden (security)

**Intermittent failures:**
- esphome, bazarr, searxng, lidarr, prowlarr
- radarr, sonarr, readarr, whisparr, node-red
- archon, litellm, ollama, open-webui
- home-assistant, zigbee2mqtt
- plex, overseerr, tautulli
- actual, paperless-ngx, unifi

**Working (CephFS-backed):**
- wizarr, threadfin (different storage class)

---

## Next Steps

### Immediate
1. ‚úÖ Execute force cleanup script for 108 volumes with watchers
2. ‚úÖ Restart CSI plugins to clear unmap queue
3. ‚úÖ Verify 62 snapshots complete successfully
4. ‚úÖ Confirm gatus/vaultwarden applications start

### Short-term
1. Commit StorageClass `discard` mount option change
2. Monitor for new orphaned volumes (should be none)
3. Update monitoring to alert on orphaned volume accumulation
4. Document cleanup procedures in runbooks

### Long-term
1. **Add automated orphaned volume detection:**
   ```bash
   # Weekly cron job to check for orphans
   # Alert if orphaned volume count > 5
   ```

2. **Consider CSI-addons for better volume management:**
   - Provides volume replication
   - Better cleanup mechanisms
   - Enhanced monitoring

3. **Evaluate alternative backup strategies:**
   - Direct VolSync copyMethod (bypasses snapshots)
   - Velero with CSI integration
   - Application-level backups where appropriate

---

## Related Issues

- **Rook Ceph Issue #134:** CSI operation lock persistence across restarts
- **Ceph-CSI Issue #2845:** No automatic cleanup of orphaned volumes (known limitation)
- **Upstream Ceph:** Stale watcher cleanup is manual operation

---

## Files Created During Investigation

### Cleanup Scripts
- `/tmp/cleanup-orphaned-rbd-volumes.sh` - Initial bulk cleanup (139 succeeded)
- `/tmp/force-cleanup-watched-volumes.sh` - Force cleanup with blacklist (108 volumes)

### Logs and Lists
- `/tmp/orphaned-volumes-verified.txt` - All 247 orphaned volumes
- `/tmp/cleanup-success.log` - 139 successfully deleted
- `/tmp/cleanup-failed.log` - 108 failed (stale watchers)
- `/tmp/cleanup-progress.log` - Execution timeline
- `/tmp/old-stuck-snapshots.txt` - 54 VolumeSnapshots >12h old
- `/tmp/k8s-volumes-fresh.txt` - Current K8s PV list
- `/tmp/ceph-volumes-fresh.txt` - Current Ceph RBD volume list

---

## Lessons Learned

1. **Always use `discard` mount option** for Ceph RBD StorageClasses
2. **Monitor orphaned volume ratio** (should always be near 0%)
3. **Stale watchers are common** after pod crashes/node failures
4. **CSI operation failures cascade** - one stuck operation can block all others
5. **External Ceph requires manual cleanup** - no automatic garbage collection

---

## Authors

Investigation conducted over 5 days by automated analysis and systematic debugging:
- Day 1-2: Snapshot deadlock identification
- Day 3: Orphaned volume discovery
- Day 4-5: Stale watcher root cause and resolution
